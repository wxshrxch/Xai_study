# 의사 결정 트리

- 질문을 던지고 답 하는 과정을 연쇄적으로 반복해 집단을 분류/예측 하는 분석법
- 답을 찾아가는 과정을 **도식적으로 표현 가능**
    - 분석 과정을 쉽게 이해 및 설명 가능
- 정보 이득 수치를 계산하여 최적 목표를 달성하는 트리 완성
    - 정보 이득 : 엔트로피의 변화량으로 계산
    - Entropy
        - 확률 변수의 평균 정보량
        - 정보를 최적으로 인코딩하기 위해 필요한 bit의 수
    - 엔트로피 계산
        
        $$
        Entropy(A) = -\sum_{k=1}^{n} p{_k} log{_2}(p{_k})
        $$
        
        - $A$ = 의사 결정을 수행하려는 전체 영역
        - $n$ = 범주 개수
        - $p{_k}$ = $A$ 영역에 속하는 레코드 가운데 $k$ 범주에 속하는 레코드의 비율
    - **정보 이득량 (Information Gain)**
        - 발생할 확률이 적을 수록 더 많은 정보량을 가지고 있음
        - $I(x) = -logP(x)$
        - 로그의 -는  P(x)의 역수를 나타냄, 정보의 희귀성에 비례
    - 한 계열에 다양한 분류 방식을 시도,  정보 이득량이 가장 커지는 방향으로 학습 → **재귀적분기(recursive partitioning)**
        - 재귀적 분기는 최대 학습 데이터의 개수만큼 발생 가능
    - 모든 학습 데이터에 대해 분기한 상태 = 풀 트리
        - 풀 트리(Full Tree) 상태는 Overfitting 우려가 큼
            - 특정 Threshold보다 낮은 수준의 정보이득이 발생하는 노드는 잘라내는 과정이  필요
                - = 프루닝(prunning)

## 의사 결정 트리 시각화

- 의사 결정 트리를 시각화 하면 해당 알고리즘이 어떤 계산 방법이나 원리를 사용했는지 이해하지 않아도 됨
- 분기점의 질문은 피쳐가 많아질 수록 구체적
    - 어떤 근거로 분기를 내는지
    - 왜 분류를 그렇게 하는지

## 피처 중요도 구하기

- Feature Importance, Permutation Importance
- 데이터의 피처가 알고리즘의 정확한 분류에 얼마나 큰 영향을 미치는가
- 특정 피처 값을 임의 값으로 치환했을 때 원래 데이터보다 예측 에러가 얼마나 더 커지는가
- **Permutation Feature Importance**
    1. 주어진 모델의 에러를 측정
    2. x의 피처 k 개에 대하여 피처 매트릭스X를 생성
    3. 피처 매트릭스 X는 피처 k를 매트릭스 X에서 임의의 값으로 변경한 모델
    4. X로 모델에러를 측정
    5. 퍼뮤테이션 피처 중요도를 산정
    6. 피처 중요도 FI를 구한다.
        - 이때, 2와 3에서 피러를 임의의 값으로 치환할 때 X는 X와 (n-1)*n번 비교가 필요
        - 전체 알고리즘이 결과를 내기까지 오래 걸린다는 단점

## 부분 의존성 플롯(PDP) 그리기

- 피처의 수치를 선형적으로 변형하면서 알고리즘 해석 능력이 얼마나 증가하고 감소하는지를 관찰하는 방식
- 피처의 값이 변할 때 모델에 미치는 영향을 가시적으로 표현 가능
- pdpbox 라이브러리로 구현되어 있음
- 부분 의존성 함수가 시각화해야 할 피처 $x_{s}$에 대하여
- $x_{s}$에 대한 주변확률분포(marginal probability density)를 계산하는 방법
    - 즉, 피처에 대한 영향력을 구하기 위해 $x_{s}$에 따라 변화하는 $x_{c}$ 피처 모든 조합을 평균하는 방식
    - 또한, 부분 의존성 함수는 몬테카를로 방식으로 근사할 수 있음

---

🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋  🍋

- 모든 인스턴스를 고려해야 함
- 예측된 결과와 모든 피처들의 global relationship 설명됨
    - PDP는 Global Method
- PDP로 표현되는 Target과 피처의 관계는
    - Linear
    - Monotonic (단조로운)
    - more complex
- PDP의 장점
    - 직관적
    - 쉬운 구현이 가능 (라이브러리 구현 완료)
    - 변수 사이 상관관계가 없다면 해석 용이
- PDP의 단점
    - 최대 2개의 피쳐에 대해서만 관계 확인 가능
    - 일정 구간에 대하여 데이터의 수가 적다면, 신뢰성이 저하
    - 데이터의 독립성 및 이질성이 취약
        - ALE(Accumuated local Effect) Plots, Individual Conditional expectation curves 로 해결 가능

---

## XGBoost 활용하기

### 1.  XGBoost의 장점

- 약한 분류기 여러개를 쌓아 복잡한 분류기를 형성
- 피처가 많지 않다면 사용하기 좋음
- 그래디언트 부스팅 라이브러리로, 병렬 처리
- Evaluaton Function 을 포함한 다양한 커스텀 최적화 옵션을 제공하여 유연성이 좋음
- Greedy 알고리즘을 사용하므로 자동적으로 Forest 가지를 친다.
    - 오버피팅으로부터 비교적 자유롭다
- XGBoost 아래로 다른 알고리즘과 섞어 앙상블 학습이 가능

### 2.  XGBoost는 딥러닝이 아니다.

### 3.  기본원리

- 트리를 만들 때 **CART(Classification And Regression Trees) 알고리즘** 이용
    - 분류 트리 분석 Classification Tree 와 회귀 트리 Regression Tree를 동시에 사용하여 트리 생성
    - 모든 리프가 모델의 최종 가치와 수치적으로 연관됨
    - 같은 분류 결과를 갖는 모델끼리도 최종 가치를 비교함으로써 모델의 우위를 선정 가능

### 피처 중요도 구분하기

- 피처 중요도 : 해당 피처가 모델의 에러를 얼마나 줄여주는 지

### 모델 튜닝하기

- Grid SerarchCV 를 통해 모델의 최적 하이퍼 파라미터를 찾는 코드
- 머신러닝 모델은 정확도, 정밀성, 재현도, 민감도, 특이성, 낙제율 등 다양한 기준에 의해 달라 질 수 있음
- 이에 모든 피처에 대한 계산은 Confusion Matrix를 사용해 찾아낼 수 있다.